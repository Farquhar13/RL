"""
Quick Facts:
    VPG is an on-policy algorithm.
    VPG can be used for environments with either discrete or continuous action spaces.
    The Spinning Up implementation of VPG supports parallelization with MPI.

Reference: https://spinningup.openai.com/en/latest/algorithms/vpg.html
"""

import numpy as np
from AgentBase import Agent
from ReplayMemory import Memory
from NeuralNetwork import PyTorchMLP
import torch

from torch.distributions.categorical import Categorical
import torch.nn as nn
from torch.optim import Adam

class PolicyGradient(Agent):

    #def __init__(self, policy, value_function, policy_lr, value_lr):
    def __init__(self, memory=None):
        if memory == None:
            self.memory = Memory(1e4)
        else:
            self.memory = memory

    def choose_action():
        """ Return the agent's action for the next time-step """
        pass

    def remember(*args):
        """ Store trajectory to in the agent's experience replay memory """
        self.memory.append(*args) # * unpacks

    def learn():
        """ Train the agent """
        pass

class CategoricalPG(Agent):
    """
    Based of off https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/2_rtg_pg.py

    Specific implementation with descrete action space and a Categorical policy.
    """

    def __init__(self, n_observations, n_actions, hidden_layer_sizes=[32], lr=1e-2):
        self.memory = Memory(1e4)
        nn_sizes = [n_observations] + hidden_layer_sizes + [n_actions]
        self.policy_net = PyTorchMLP(nn_sizes, activation=nn.Tanh, output_activation=nn.Identity)
        self.optimizer = Adam(self.policy_net.parameters(), lr=lr)
        self.episode_rewards = []

    def get_policy(self, observation):
        """ Returns output of self.policy_net """
        logits = self.policy_net(observation)
        return Categorical(logits=logits)

    def choose_action(self, observation):
        """ Return the agent's action for the next time-step """
        if isinstance(observation, (np.ndarray, list)):
            observation = torch.tensor(observation, dtype=torch.float32)

        return self.get_policy(observation).sample().item() # item() to return scalar from torch.tensor

    def compute_loss(self, observation, actions, weights):
        """ A loss function whose gradient is the policy gradient (for the data generated by the policy).
        Inputs: must all be tensors.
        Returns: A torch.tensor with a single element (the loss).
        """
        logp = self.get_policy(observation).log_prob(actions)
        return -(logp * weights).mean()

    def remember(self, *args):
        """ Store trajectory to in the agent's experience replay memory.

        Example arguments are: observation, action, next_observation, reward --
        as those given by the OpenAI env.step() function
        """
        self.memory.remember(*args)

    def compute_rewards_to_go(self, rewards):
        """ Returns: (np.ndarray) where element i is the sum of rewards from time-step i until the
        end of the episode. """
        total_reward = sum(rewards)
        reward_to_go = np.zeros_like(rewards)
        for i in range(len(rewards)):
            if i != 0:
                total_reward -= rewards[i-1]
            reward_to_go[i] = total_reward

        return reward_to_go

    def learn(self):
        """ Train the agent """
        # Get batch of experiences
        observations = [m[0] for m in self.memory]
        actions = [m[1] for m in self.memory]

        # Compute rewards-to-go
        rewards_to_go = []
        for ep_rewards in self.episode_rewards:
            rtg = self.compute_rewards_to_go(ep_rewards)
            rewards_to_go += list(self.compute_rewards_to_go(ep_rewards))

        # Take a single policy gradient update step
        self.optimizer.zero_grad()
        batch_loss = self.compute_loss(torch.as_tensor(observations, dtype=torch.float32),
                                  torch.as_tensor(actions, dtype=torch.int32),
                                  torch.as_tensor(rewards_to_go, dtype=torch.float32))
        batch_loss.backward()
        self.optimizer.step()

        # Clear memory
        self.memory.memory.clear()
        self.episode_rewards = []

        return batch_loss

class RTG_Policy_Gradient(Agent):
    """
    RTG = Reward-to-go.

    A more general abstract class for Policy Gradient algorithms.

    - The "actor" (policy) is a combination of a model (usually a PyTorch
    neural network) and a PyTorch distribution. https://pytorch.org/docs/stable/distributions.html

    - The "critic" (value-estimator) is the reward-to-go function. The future-return from a given time-step given by the
    agent's experience in sampling from the environment.
    """

    def __init__(self, n_observations, n_actions,
                 policy_net=None,
                 memory=Memory(1e4),
                 distribution=Categorical,
                 optimizer=Adam,
                 hidden_layer_sizes=[32],
                 lr=1e-2):
        self.memory = memory
        self.distribution = distribution

        nn_sizes = [n_observations] + hidden_layer_sizes + [n_actions]
        if policy_net == None:
            self.policy_net = PyTorchMLP(nn_sizes, activation=nn.Tanh, output_activation=nn.Identity)
        else:
            self.policy_net = policy_net

        self.optimizer = optimizer(self.policy_net.parameters(), lr=lr)

    def get_policy(self, observation):
        """ Returns the output of calling the distribution """
        raise NotImplementedError

    def choose_action(self, observation):
        """ Return the agent's action for the next time-step """
        if isinstance(observation, (np.ndarray, list)):
            observation = torch.tensor(observation, dtype=torch.float32)

        raise NotImplementedError

    def compute_loss(self, observation, actions, weights):
        """ A loss function whose gradient is the policy gradient (for the data generated by the policy).
        Inputs: must all be tensors.
        Returns: A torch.tensor with a single element (the loss).
        """
        logp = self.get_policy(observation).log_prob(actions)
        return -(logp * weights).mean()

    def remember(self, *args):
        """ Store trajectory to in the agent's experience replay memory.

        Example arguments are: observation, action, next_observation, reward, done --
        as those given by the OpenAI env.step() function
        """
        self.memory.remember(*args)

    def compute_rewards_to_go(self, rewards):
        """ Returns: (np.ndarray) where element i is the sum of rewards from time-step i until the
        end of the episode. """
        total_reward = sum(rewards)
        reward_to_go = np.zeros_like(rewards)
        for i in range(len(rewards)):
            if i != 0:
                total_reward -= rewards[i-1]
            reward_to_go[i] = total_reward

        return reward_to_go

    def partition_by_episode(self, data, done_list):
        """ Paritions the data for all expereiences into lists corresponding to episodes """
        data_by_episode = []

        episode_data = []
        for datum, done in zip(data, done_list):
            episode_data.append(datum)

            if done == True:
                data_by_episode.append(episode_data)
                episode_data = []

        return data_by_episode

    def learn(self):
        """ Train the agent """
        # Get batch of experiences
        observations = [m[0] for m in self.memory]
        actions = [m[1] for m in self.memory]
        rewards = [m[3] for m in self.memory]
        episode_is_done_list = [m[4] for m in self.memory]

        # Compute rewards-to-go
        episode_rewards = self.partition_by_episode(rewards, episode_is_done_list)
        rewards_to_go = []
        for ep_rewards in episode_rewards:
            rewards_to_go += list(self.compute_rewards_to_go(ep_rewards))

        # Take a single policy gradient update step
        self.optimizer.zero_grad()
        batch_loss = self.compute_loss(torch.as_tensor(observations, dtype=torch.float32),
                                  torch.as_tensor(actions, dtype=torch.int32),
                                  torch.as_tensor(rewards_to_go, dtype=torch.float32))
        batch_loss.backward()
        self.optimizer.step()

        # Clear memory
        self.memory.memory.clear()
        self.episode_rewards = []

        return batch_loss

class PolicyGradient(Agent):
    """
    An abstract class for the vanilla policy gradient algorithm which can be initialized with
    a variety of actors and critics.

    - The "actor" (policy) is a combination of a model (usually a PyTorch
    neural network) and a PyTorch distribution. https://pytorch.org/docs/stable/distributions.html

    - The "critic" is an estimator of the Value or Advantage of a state. Must be implemented in
    the critic() function.
    """

    def __init__(self, n_observations, n_actions,
                 policy_net=None,
                 default_hidden_layer_sizes=[32],
                 lr=1e-2,
                 optimizer=Adam,
                 memory=Memory(1e4),
                 distribution=Categorical,
                 value_net=None):
        self.memory = memory
        self.distribution = distribution

        if policy_net == None:
            nn_sizes = [n_observations] + default_hidden_layer_sizes + [n_actions]
            self.policy_net = PyTorchMLP(nn_sizes, activation=nn.Tanh, output_activation=nn.Identity)
        else:
            self.policy_net = policy_net
        self.optimizer = optimizer(self.policy_net.parameters(), lr=lr)

        self.value_net = value_net

    def get_batch(self):
        n_recorded_things_per_episode = len(self.memory[0])
        recorded_things = []
        for i in range(n_recorded_things_per_episode):
            recorded_things.append([m[i] for m in self.memory])

        return recorded_things

    def train_value_net(self):
        """ Called by learn() if a value_net attribute is provided. """
        raise NotImplementedError

    def critic(self):
        """ Return the value-estimation for each training example as a np.ndarray """
        raise NotImplementedError

    def get_policy(self, observation):
        """ Returns output of self.policy_net """
        raise NotImplementedError

    def choose_action(self, observation):
        """ Return the agent's action for the next time-step """
        if isinstance(observation, (np.ndarray, list)):
            observation = torch.tensor(observation, dtype=torch.float32)

        raise NotImplementedError

    def compute_loss(self, observation, actions, weights):
        """ A loss function whose gradient is the policy gradient (for the data generated by the policy).
        Inputs: must all be tensors.
        Returns: A torch.tensor with a single element (the loss).
        """
        logp = self.get_policy(observation).log_prob(actions)
        return -(logp * weights).mean()

    def remember(self, *args):
        """ Store trajectory to in the agent's experience replay memory.

        Example arguments are: observation, action, next_observation, reward, done--
        as those given by the OpenAI env.step() function.

        Note: it is critical to the learn() funciton that the first two arguments
        are observation and action.
        """
        self.memory.remember(*args)

    def partition_by_episode(self, data, done_list):
        """ Paritions the data for all expereiences into lists corresponding to episodes """
        data_by_episode = []

        episode_data = []
        for datum, done in zip(data, done_list):
            episode_data.append(datum)

            if done == True:
                data_by_episode.append(episode_data)
                episode_data = []

        return data_by_episode

    def learn(self):
        """ Train the agent """
        # Synchronize updates
        if self.value_net is not None:
            self.train_value_net()

        observations = [m[0] for m in self.memory]
        actions = [m[1] for m in self.memory]
        critic_evaluation = self.critic()

        # Take a single policy gradient update step
        self.optimizer.zero_grad()
        batch_loss = self.compute_loss(torch.as_tensor(observations, dtype=torch.float32),
                                  torch.as_tensor(actions, dtype=torch.int32),
                                  torch.as_tensor(critic_evaluation, dtype=torch.float32))
        batch_loss.backward()
        self.optimizer.step()

        # Clear memory
        self.memory.memory.clear()
        self.episode_rewards = []

        return batch_loss

if __name__ == "__main__":
    CPG = CategoricalPG(8, 4)
    x = torch.rand(8)
    print(x)
    print(CPG.get_policy(x))
    print(CPG.choose_action(x))
    print("loss", CPG.compute_loss(x, torch.tensor(0), 1))
    print(type(CPG.compute_loss(x, torch.tensor(0), 1)))

    c = Categorical(torch.tensor([0.25]*4))
    print(c)
    print(c.sample())
    print(c.log_prob(torch.tensor(0)))
    print(np.log(0.25))

    print()
    x = np.random.rand(8)
    print(CPG.choose_action(x))


    print()
    x = [1,2,3,4,5,6.,7,8]
    print(CPG.choose_action(x))
